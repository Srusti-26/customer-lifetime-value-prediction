{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Customer Lifetime Value (LTV) Prediction Model\n",
    "\n",
    "This notebook implements a complete LTV prediction pipeline using XGBoost regression with proper validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "try:\n",
    "    # Try to load existing RFM features\n",
    "    rfm_data = pd.read_csv('../notebooks/rfm_features.csv')\n",
    "    print(f\"Loaded RFM features: {rfm_data.shape}\")\n",
    "except:\n",
    "    # If RFM features don't exist, create them from raw data\n",
    "    print(\"Creating RFM features from raw data...\")\n",
    "    \n",
    "    # Load raw data\n",
    "    retail_data = pd.read_csv('../Data/OnlineRetail.csv', encoding='ISO-8859-1', parse_dates=['InvoiceDate'])\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    retail_data['TotalAmount'] = retail_data['Quantity'] * retail_data['UnitPrice']\n",
    "    retail_data = retail_data.dropna(subset=['CustomerID'])\n",
    "    retail_data['CustomerID'] = retail_data['CustomerID'].astype(int)\n",
    "    retail_data = retail_data[retail_data['UnitPrice'] > 0]\n",
    "    \n",
    "    # Reference date for RFM calculation\n",
    "    reference_date = retail_data['InvoiceDate'].max() + timedelta(days=1)\n",
    "    \n",
    "    # Create RFM features\n",
    "    rfm_data = retail_data.groupby('CustomerID').agg({\n",
    "        'InvoiceDate': lambda x: (reference_date - x.max()).days,  # Recency\n",
    "        'InvoiceNo': 'nunique',                                    # Frequency\n",
    "        'TotalAmount': 'sum'                                       # Monetary\n",
    "    }).rename(columns={'InvoiceDate': 'Recency', 'InvoiceNo': 'Frequency', 'TotalAmount': 'Monetary'})\n",
    "    \n",
    "    # Additional features\n",
    "    # Average Order Value\n",
    "    aov = retail_data.groupby(['CustomerID', 'InvoiceNo'])['TotalAmount'].sum().groupby('CustomerID').mean().reset_index()\n",
    "    aov.columns = ['CustomerID', 'AvgOrderValue']\n",
    "    rfm_data = rfm_data.merge(aov, on='CustomerID')\n",
    "    \n",
    "    # Customer tenure\n",
    "    tenure = retail_data.groupby('CustomerID')['InvoiceDate'].agg(['min', 'max']).reset_index()\n",
    "    tenure['Tenure'] = (tenure['max'] - tenure['min']).dt.days\n",
    "    rfm_data = rfm_data.merge(tenure[['CustomerID', 'Tenure']], on='CustomerID')\n",
    "    \n",
    "    # Product diversity\n",
    "    diversity = retail_data.groupby('CustomerID')['StockCode'].nunique().reset_index()\n",
    "    diversity.columns = ['CustomerID', 'ProductDiversity']\n",
    "    rfm_data = rfm_data.merge(diversity, on='CustomerID')\n",
    "    \n",
    "    rfm_data = rfm_data.reset_index()\n",
    "    print(f\"Created RFM features: {rfm_data.shape}\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nDataset Info:\")\n",
    "print(rfm_data.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(rfm_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering and Target Variable Creation\n",
    "\n",
    "# Create target variable (LTV) - we'll use Monetary as our LTV proxy\n",
    "# In a real scenario, you might want to predict future value based on historical data\n",
    "rfm_data['LTV'] = rfm_data['Monetary']\n",
    "\n",
    "# Create additional derived features\n",
    "rfm_data['RecencyScore'] = pd.qcut(rfm_data['Recency'], 5, labels=[5,4,3,2,1], duplicates='drop')\n",
    "rfm_data['FrequencyScore'] = pd.qcut(rfm_data['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5], duplicates='drop')\n",
    "rfm_data['MonetaryScore'] = pd.qcut(rfm_data['Monetary'].rank(method='first'), 5, labels=[1,2,3,4,5], duplicates='drop')\n",
    "\n",
    "# Convert categorical scores to numeric\n",
    "rfm_data['RecencyScore'] = pd.to_numeric(rfm_data['RecencyScore'], errors='coerce')\n",
    "rfm_data['FrequencyScore'] = pd.to_numeric(rfm_data['FrequencyScore'], errors='coerce')\n",
    "rfm_data['MonetaryScore'] = pd.to_numeric(rfm_data['MonetaryScore'], errors='coerce')\n",
    "\n",
    "# Fill any NaN values\n",
    "rfm_data = rfm_data.fillna(0)\n",
    "\n",
    "# Create composite features\n",
    "rfm_data['RFM_Score'] = rfm_data['RecencyScore'] + rfm_data['FrequencyScore'] + rfm_data['MonetaryScore']\n",
    "rfm_data['FrequencyMonetaryRatio'] = rfm_data['Frequency'] / (rfm_data['Monetary'] + 1)\n",
    "rfm_data['AvgDaysBetweenPurchases'] = rfm_data['Tenure'] / (rfm_data['Frequency'] + 1)\n",
    "\n",
    "print(\"Feature engineering completed!\")\n",
    "print(f\"Dataset shape: {rfm_data.shape}\")\n",
    "print(f\"Features: {list(rfm_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "\n",
    "# Select features for modeling (exclude target and ID columns)\n",
    "feature_columns = ['Recency', 'Frequency', 'AvgOrderValue', 'Tenure', 'ProductDiversity',\n",
    "                  'RecencyScore', 'FrequencyScore', 'MonetaryScore', 'RFM_Score',\n",
    "                  'FrequencyMonetaryRatio', 'AvgDaysBetweenPurchases']\n",
    "\n",
    "# Ensure all feature columns exist\n",
    "available_features = [col for col in feature_columns if col in rfm_data.columns]\n",
    "print(f\"Available features: {available_features}\")\n",
    "\n",
    "X = rfm_data[available_features]\n",
    "y = rfm_data['LTV']\n",
    "\n",
    "# Remove outliers (optional - using IQR method)\n",
    "Q1 = y.quantile(0.25)\n",
    "Q3 = y.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Keep data within bounds\n",
    "mask = (y >= lower_bound) & (y <= upper_bound)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(f\"Data shape after outlier removal: {X.shape}\")\n",
    "print(f\"Target variable statistics:\")\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and scale features\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "# Scale features for some models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'predictions': y_pred}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# 1. Linear Regression (baseline)\n",
    "print(\"Training Linear Regression...\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "results['Linear Regression'] = evaluate_model(lr_model, X_test_scaled, y_test, 'Linear Regression')\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "results['Random Forest'] = evaluate_model(rf_model, X_test, y_test, 'Random Forest')\n",
    "\n",
    "# 3. XGBoost\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "results['XGBoost'] = evaluate_model(xgb_model, X_test, y_test, 'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for Best Model\n",
    "\n",
    "# Find the best model based on R² score\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['R2'])\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "\n",
    "# Hyperparameter tuning for XGBoost (assuming it's competitive)\n",
    "print(\"\\nPerforming hyperparameter tuning for XGBoost...\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Use a smaller parameter grid for faster execution\n",
    "xgb_grid = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_grid, \n",
    "    param_grid, \n",
    "    cv=3, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {-grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Evaluate the tuned model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "results['XGBoost Tuned'] = evaluate_model(best_xgb_model, X_test, y_test, 'XGBoost Tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison and Visualization\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'MAE': [results[model]['MAE'] for model in results.keys()],\n",
    "    'RMSE': [results[model]['RMSE'] for model in results.keys()],\n",
    "    'R2': [results[model]['R2'] for model in results.keys()]\n",
    "})\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Visualize model performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# MAE comparison\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['MAE'])\n",
    "axes[0].set_title('Mean Absolute Error (MAE)')\n",
    "axes[0].set_ylabel('MAE')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[1].bar(comparison_df['Model'], comparison_df['RMSE'])\n",
    "axes[1].set_title('Root Mean Square Error (RMSE)')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# R² comparison\n",
    "axes[2].bar(comparison_df['Model'], comparison_df['R2'])\n",
    "axes[2].set_title('R² Score')\n",
    "axes[2].set_ylabel('R² Score')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "\n",
    "# Get the best model (assuming XGBoost Tuned is best)\n",
    "final_model = best_xgb_model\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': available_features,\n",
    "    'importance': final_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "plt.title('Feature Importance (XGBoost)')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Final LTV Predictions\n",
    "\n",
    "# Make predictions for all customers\n",
    "all_predictions = final_model.predict(X)\n",
    "\n",
    "# Create final results dataframe\n",
    "final_results = rfm_data[['CustomerID']].copy()\n",
    "final_results['Actual_LTV'] = y.values\n",
    "final_results['Predicted_LTV'] = all_predictions\n",
    "final_results['LTV_Difference'] = final_results['Predicted_LTV'] - final_results['Actual_LTV']\n",
    "final_results['LTV_Accuracy'] = 1 - abs(final_results['LTV_Difference']) / final_results['Actual_LTV']\n",
    "\n",
    "# Add customer segments based on predicted LTV\n",
    "final_results['LTV_Segment'] = pd.qcut(\n",
    "    final_results['Predicted_LTV'], \n",
    "    q=5, \n",
    "    labels=['Low Value', 'Medium-Low', 'Medium', 'Medium-High', 'High Value'],\n",
    "    duplicates='drop'\n",
    ")\n",
    "\n",
    "# Add RFM features for context\n",
    "final_results = final_results.merge(\n",
    "    rfm_data[['CustomerID', 'Recency', 'Frequency', 'Monetary', 'AvgOrderValue']], \n",
    "    on='CustomerID'\n",
    ")\n",
    "\n",
    "print(\"Final Results Summary:\")\n",
    "print(final_results.describe())\n",
    "\n",
    "print(\"\\nLTV Segments Distribution:\")\n",
    "print(final_results['LTV_Segment'].value_counts())\n",
    "\n",
    "# Save final predictions\n",
    "final_results.to_csv('../notebooks/final_ltv_predictions.csv', index=False)\n",
    "print(\"\\nFinal LTV predictions saved to 'final_ltv_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Visualizations\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Actual vs Predicted LTV\n",
    "axes[0, 0].scatter(final_results['Actual_LTV'], final_results['Predicted_LTV'], alpha=0.6)\n",
    "axes[0, 0].plot([final_results['Actual_LTV'].min(), final_results['Actual_LTV'].max()], \n",
    "                [final_results['Actual_LTV'].min(), final_results['Actual_LTV'].max()], 'r--')\n",
    "axes[0, 0].set_xlabel('Actual LTV')\n",
    "axes[0, 0].set_ylabel('Predicted LTV')\n",
    "axes[0, 0].set_title('Actual vs Predicted LTV')\n",
    "\n",
    "# 2. LTV Segment Distribution\n",
    "segment_counts = final_results['LTV_Segment'].value_counts()\n",
    "axes[0, 1].pie(segment_counts.values, labels=segment_counts.index, autopct='%1.1f%%')\n",
    "axes[0, 1].set_title('Customer LTV Segments')\n",
    "\n",
    "# 3. Prediction Accuracy Distribution\n",
    "axes[1, 0].hist(final_results['LTV_Accuracy'].dropna(), bins=30, alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Prediction Accuracy')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('LTV Prediction Accuracy Distribution')\n",
    "\n",
    "# 4. LTV by Segment\n",
    "sns.boxplot(data=final_results, x='LTV_Segment', y='Predicted_LTV', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Predicted LTV by Segment')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/final_ltv_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== LTV PREDICTION MODEL COMPLETED ===\")\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Final Model MAE: {results[list(results.keys())[-1]]['MAE']:.2f}\")\n",
    "print(f\"Final Model RMSE: {results[list(results.keys())[-1]]['RMSE']:.2f}\")\n",
    "print(f\"Final Model R²: {results[list(results.keys())[-1]]['R2']:.4f}\")\n",
    "print(f\"Total Customers Analyzed: {len(final_results)}\")\n",
    "print(\"\\nDeliverables Created:\")\n",
    "print(\"✓ Trained XGBoost model with hyperparameter tuning\")\n",
    "print(\"✓ Model validation with MAE, RMSE metrics\")\n",
    "print(\"✓ Final LTV predictions CSV\")\n",
    "print(\"✓ Customer segmentation based on predicted LTV\")\n",
    "print(\"✓ Comprehensive visualizations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}